# GoDeMode: Code Generation vs Native Tool Calling Benchmark

A comprehensive benchmark comparing **Code Mode** (LLM-generated Go code execution) vs **Native Tool Calling** (direct function invocation) for IT support automation tasks using Claude API.

[![Go 1.21+](https://img.shields.io/badge/go-1.21+-blue)]()

## ğŸ¯ What is This?

This project benchmarks two approaches to building agentic AI systems:

1. **Code Mode**: Claude generates complete Go programs that are interpreted and executed
2. **Native Tool Calling**: Claude makes sequential tool calls using Anthropic's tool use API

Both approaches solve the same tasks using the same underlying tools, allowing direct performance comparison.

## âœ¨ Features

### Benchmark Framework
- âœ… **3 Complexity Levels**: Simple (3 ops) â†’ Medium (8 ops) â†’ Complex (15 ops)
- âœ… **5 Real Systems**: Email, SQLite, Knowledge Graph, Logs, Configs
- âœ… **18 Production Tools**: Real operations across all systems
- âœ… **Full Verification**: SQL queries, file checks, graph validation
- âœ… **Complete Metrics**: Duration, tokens, API calls, success rates
- âœ… **Side-by-Side Comparison**: Both modes pass all verifications
- âœ… **Claude API Integration**: Uses claude-sonnet-4-20250514

### Code Mode Implementation
- âœ… **yaegi Interpreter**: Fast Go code interpretation without compilation
- âœ… **Source Validation**: Blocks dangerous imports and operations
- âœ… **Execution Timeouts**: Context-based cancellation (30s default)
- âœ… **Parameter Extraction**: Intelligent parsing of generated code for actual tool execution

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install Go 1.21+
go version

# Set Claude API key
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Run the Benchmark

```bash
# Build and run
go build -o godemode-benchmark benchmark/cmd/main.go
./godemode-benchmark

# Or run specific complexity
TASK_FILTER=medium ./godemode-benchmark
```

### Expected Output

```
=== Running Task: email-to-ticket ===

--- Running CODE MODE Agent ---
Generated code solves task in single API call...

--- Running FUNCTION CALLING Agent ---
Step-by-step tool calls...

====================================================================================================
BENCHMARK REPORT
====================================================================================================
1. email-to-ticket (simple, 3 operations)
   CODE MODE:         âœ“ All checks passed (11s, 1,448 tokens, 1 API call)
   FUNCTION CALLING:  âœ“ All checks passed (13s, 2,764 tokens, 4 API calls)
   COMPARISON: Code Mode 19% faster, used 1,316 fewer tokens, made 3 fewer API calls
```

## ğŸ“Š Latest Benchmark Results

All 3 tasks pass verification for both approaches âœ…

| Task | Complexity | Code Mode | Function Calling | Advantage |
|------|------------|-----------|------------------|-----------|
| Email to Ticket | Simple (3 ops) | âœ… 11s, 1.4K tokens, 1 call | âœ… 13s, 2.8K tokens, 4 calls | Code Mode |
| Investigate Logs | Medium (8 ops) | âœ… 33s, 3.1K tokens, 1 call | âœ… 28s, 6.7K tokens, 8 calls | Function Calling (speed) / Code Mode (efficiency) |
| Auto-Resolution | Complex (15 ops) | âœ… 36s, 4.0K tokens, 1 call | âœ… 51s, 13.4K tokens, 15 calls | Code Mode |

### Key Insights

**Code Mode Advantages:**
- ğŸ“‰ **50-70% fewer tokens** - Single LLM call vs iterative approach
- ğŸ“‰ **75-93% fewer API calls** - 1 call vs 4-15 calls
- ğŸ‘ï¸ **Full code visibility** - See complete program logic
- ğŸ§  **Better planning** - Holistic approach to complex tasks
- ğŸ’° **Lower cost** - Significant token and API call savings

**Function Calling Advantages:**
- âš¡ **Faster on medium tasks** - No interpretation overhead for simple operations
- ğŸ¯ **More predictable** - Exactly expected number of operations
- ğŸ”„ **Easier debugging** - Step-by-step execution visibility
- ğŸ’ª **More reliable** - Handles errors gracefully with partial completion

## ğŸ—ï¸ Architecture

```
godemode/
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ agents/                   # CodeMode & FunctionCalling implementations
â”‚   â”‚   â”œâ”€â”€ codemode_agent.go
â”‚   â”‚   â””â”€â”€ function_calling_agent.go
â”‚   â”œâ”€â”€ systems/                  # Real systems (Email, DB, Graph, Logs, Config)
â”‚   â”œâ”€â”€ tools/                    # 18 tool implementations
â”‚   â”œâ”€â”€ scenarios/                # 3 tasks with setup & verification
â”‚   â”œâ”€â”€ runner/                   # Benchmark orchestration & reporting
â”‚   â”œâ”€â”€ llm/                      # Claude API integration
â”‚   â””â”€â”€ cmd/main.go              # Main benchmark executable
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ compiler/                 # Code compilation (cached)
â”‚   â”œâ”€â”€ validator/                # Safety validation
â”‚   â””â”€â”€ executor/                 # yaegi interpreter executor
â””â”€â”€ examples/                     # Example programs
```

## ğŸ”§ Integration with Claude API

### Set API Key
```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Model Selection
```bash
# Use Sonnet 4 (default, recommended)
./godemode-benchmark

# Or specify model
CLAUDE_MODEL=claude-opus-4-20250514 ./godemode-benchmark
```

## ğŸ“ How It Works

### Code Mode Flow
1. Claude generates complete Go program using task description
2. Code is validated for dangerous operations
3. yaegi interpreter executes the code
4. Tool calls are extracted and executed against real systems
5. Results are verified

### Function Calling Flow
1. Claude creates step-by-step plan
2. For each step, Claude decides which tool to call
3. Tool is executed against real systems
4. Result is fed back to Claude
5. Process repeats until task complete

## ğŸ”’ Security Features

### Blocked by Validator:
- âŒ `os/exec` - Command execution
- âŒ `syscall` - System calls
- âŒ `unsafe` - Unsafe operations
- âŒ `net` - Network access
- âŒ `plugin` - Dynamic loading

### Execution Constraints:
- â±ï¸ 30-second timeout per task
- ğŸ” Interpreted execution (no system compilation)
- ğŸ“ No direct file system access (only through provided APIs)

## ğŸ§ª Testing

```bash
# Run full benchmark
./godemode-benchmark

# Run specific complexity level
TASK_FILTER=simple ./godemode-benchmark
TASK_FILTER=medium ./godemode-benchmark
TASK_FILTER=complex ./godemode-benchmark

# Run unit tests
go test ./...
```

## ğŸ¯ Use Cases

### When to Use Code Mode
- âœ… Need to minimize API calls and tokens
- âœ… Complex workflows with loops/conditionals
- âœ… Cost optimization is priority
- âœ… Full code audit trail desired

### When to Use Function Calling
- âœ… Need predictable operation counts
- âœ… Real-time responses important
- âœ… Debugging visibility critical
- âœ… Simpler implementation preferred

## ğŸš§ Current Status

### Completed
- [x] yaegi interpreter-based execution
- [x] Source validation
- [x] 5 real systems with 18 tools
- [x] 3 benchmark scenarios (simple, medium, complex)
- [x] Full verification for both modes
- [x] Claude API integration
- [x] Both agents passing 100% of tests
- [x] Comprehensive metrics collection

### Future Work
- [ ] Additional benchmark scenarios
- [ ] Performance optimizations
- [ ] Additional LLM provider support
- [ ] Enhanced security validations
- [ ] MCP (Model Context Protocol) integration

## ğŸ¤ Contributing

Areas for contribution:
- Additional benchmark scenarios
- More tool implementations
- Performance optimizations
- Additional LLM providers
- Documentation improvements

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- [yaegi](https://github.com/traefik/yaegi) - Go interpreter
- [Anthropic Claude](https://www.anthropic.com/) - LLM capabilities
- [SQLite](https://www.sqlite.org/) - Database
- [BadgerDB](https://github.com/dgraph-io/badger) - Knowledge graph storage

---

**Built with â¤ï¸ using Go and Claude API**

*Production-ready benchmark framework for comparing agentic AI approaches*
